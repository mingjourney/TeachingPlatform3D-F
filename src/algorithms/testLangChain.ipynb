{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "from langchain import OpenAI\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ„å»ºè¯­è¨€æ¨¡å‹åº”ç”¨ç¨‹åº: LLMåˆ†æä¸€ä¸‹è¿™ä¸ª\n",
      "é—®é¢˜\n",
      "\n",
      "è¯­è¨€æ¨¡å‹æ˜¯ä¸€ç§åŸºäºæ¦‚ç‡çš„æ¨¡å‹ï¼Œå®ƒå¯ä»¥ç”¨æ¥é¢„æµ‹ä¸€ä¸ªå¥å­æˆ–æ–‡æœ¬åºåˆ—çš„æ¦‚ç‡ã€‚å®ƒå¯ä»¥å¸®åŠ©æˆ‘ä»¬ç†è§£è¯­è¨€çš„ç»“æ„å’Œè§„å¾‹ï¼Œä»è€Œæé«˜è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡çš„æ€§èƒ½ã€‚\n",
      "\n",
      "åœ¨æ„å»ºè¯­è¨€æ¨¡å‹åº”ç”¨ç¨‹åºæ—¶ï¼Œæˆ‘ä»¬éœ€è¦è€ƒè™‘ä»¥ä¸‹å‡ ä¸ªæ–¹é¢ï¼š\n",
      "\n",
      "1. æ•°æ®æ”¶é›†å’Œé¢„å¤„ç†ï¼šé¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦æ”¶é›†å¤§é‡çš„æ–‡æœ¬æ•°æ®ä½œä¸ºè¯­æ–™åº“ï¼Œè¿™äº›æ•°æ®å¯ä»¥æ¥è‡ªäºå„ç§æ¥æºï¼Œå¦‚æ–°é—»ã€ç¤¾äº¤åª’ä½“ã€ç½‘é¡µç­‰ã€‚ç„¶åï¼Œæˆ‘ä»¬éœ€è¦å¯¹è¿™äº›æ•°æ®è¿›è¡Œé¢„å¤„ç†ï¼ŒåŒ…æ‹¬åˆ†è¯ã€å»é™¤åœç”¨è¯ã€è¯å¹²åŒ–ç­‰ï¼Œä»¥ä¾¿äºåç»­çš„å»ºæ¨¡å’Œåˆ†æã€‚\n",
      "\n",
      "2. é€‰æ‹©æ¨¡å‹ï¼šè¯­è¨€æ¨¡å‹æœ‰å¤šç§ç±»å‹ï¼Œå¦‚n-gramæ¨¡å‹ã€ç¥ç»ç½‘ç»œæ¨¡å‹ç­‰ã€‚æˆ‘ä»¬éœ€è¦æ ¹æ®å…·ä½“çš„åº”ç”¨åœºæ™¯é€‰æ‹©åˆé€‚çš„æ¨¡å‹ã€‚ä¾‹å¦‚ï¼Œå¦‚æœ\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"]=\"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\"\n",
    "# å®šä¹‰å­—ç¬¦ä¸²æ¨¡æ¿\n",
    "template = \"{txt}åˆ†æä¸€ä¸‹è¿™ä¸ª\"\n",
    "#å®ä¾‹åŒ–æ¨¡æ¿\n",
    "prompt_temp = PromptTemplate.from_template(template)\n",
    "# æ ¹æ®æ¨¡æ¿ç”Ÿæˆæç¤ºè¯\n",
    "prompt = prompt_temp.format(txt=\"æ„å»ºè¯­è¨€æ¨¡å‹åº”ç”¨ç¨‹åº: LLM\")\n",
    "print(prompt)\n",
    "model = OpenAI(temperature=0)\n",
    "result = model(prompt)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gujiaming/Library/Python/3.8/lib/python/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `run` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\n\"Rainbow Threads\"'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import LLMChain\n",
    "llm = OpenAI(temperature=0.9)\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"product\"],\n",
    "    template=\"What is a good name for a company that makes {product}?\",\n",
    ")\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "chain.run(\"colorful socks\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "èŠå¤©æ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"J'adore la programmation.\", response_metadata={'token_usage': {'completion_tokens': 8, 'prompt_tokens': 19, 'total_tokens': 27}, 'model_name': 'gpt-3.5-turbo-1106', 'system_fingerprint': 'fp_592ef5907d', 'finish_reason': 'stop', 'logprobs': None}, id='run-556d8cc8-8277-48aa-bf70-731f88ca9571-0')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage\n",
    "chat = ChatOpenAI(model=\"gpt-3.5-turbo-1106\", temperature=0.2)\n",
    "\n",
    "chat.invoke(\n",
    "    [\n",
    "        HumanMessage(\n",
    "            content=\"Translate this sentence from English to French: I love programming.\"\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='I said \"J\\'adore la programmation\" which means \"I love programming\" in French.', response_metadata={'token_usage': {'completion_tokens': 21, 'prompt_tokens': 41, 'total_tokens': 62}, 'model_name': 'gpt-3.5-turbo-1106', 'system_fingerprint': 'fp_592ef5907d', 'finish_reason': 'stop', 'logprobs': None}, id='run-e674910a-8d3a-4b51-b691-c9e99def2db7-0')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage\n",
    "\n",
    "chat.invoke(\n",
    "    [\n",
    "        HumanMessage(\n",
    "            content=\"Translate this sentence from English to French: I love programming.\"\n",
    "        ),\n",
    "        AIMessage(content=\"J'adore la programmation.\"),\n",
    "        HumanMessage(content=\"What did you just say?\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ChatMessageHistory\n",
    "demo_ephemeral_chat_history = ChatMessageHistory()\n",
    "\n",
    "demo_ephemeral_chat_history.add_user_message(\"hi!\")\n",
    "\n",
    "demo_ephemeral_chat_history.add_ai_message(\"whats up?\")\n",
    "\n",
    "demo_ephemeral_chat_history.add_user_message(\n",
    "    \"what did i just say\"\n",
    ")\n",
    "\n",
    "response = chat.invoke(demo_ephemeral_chat_history.messages)\n",
    "demo_ephemeral_chat_history.add_ai_message(response)\n",
    "\n",
    "demo_ephemeral_chat_history.add_user_message(\n",
    "    \"what did you just say\"\n",
    ")\n",
    "response = chat.invoke(demo_ephemeral_chat_history.messages)\n",
    "demo_ephemeral_chat_history.add_ai_message(response)\n",
    "\n",
    "demo_ephemeral_chat_history.messages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "loader = WebBaseLoader(\"https://docs.smith.langchain.com/overview\")\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "all_splits = text_splitter.split_documents(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents=all_splits, embedding=OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Getting started with LangSmith | ğŸ¦œï¸ğŸ› ï¸ LangSmith', metadata={'description': 'Introduction', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'Getting started with LangSmith | ğŸ¦œï¸ğŸ› ï¸ LangSmith'}),\n",
       " Document(page_content='Skip to main contentLangSmith API DocsSearchGo to AppQuick StartUser GuideTracingEvaluationProduction Monitoring & AutomationsPrompt HubProxyPricingSelf-HostingCookbookQuick StartOn this pageGetting started with LangSmithIntroduction\\u200bLangSmith is a platform for building production-grade LLM applications. It allows you to closely monitor and evaluate your application, so you can ship quickly and with confidence. Use of LangChain is not necessary - LangSmith works on its own!Install LangSmith\\u200bWe', metadata={'description': 'Introduction', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'Getting started with LangSmith | ğŸ¦œï¸ğŸ› ï¸ LangSmith'}),\n",
       " Document(page_content='LangSmith.Self-Hosting: Learn about self-hosting options for LangSmith.Proxy: Learn about the proxy capabilities of LangSmith.Tracing: Learn about the tracing capabilities of LangSmith.Evaluation: Learn about the evaluation capabilities of LangSmith.Prompt Hub Learn about the Prompt Hub, a prompt management tool built into LangSmith.Additional Resources\\u200bLangSmith Cookbook: A collection of tutorials and end-to-end walkthroughs using LangSmith.LangChain Python: Docs for the Python LangChain', metadata={'description': 'Introduction', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'Getting started with LangSmith | ğŸ¦œï¸ğŸ› ï¸ LangSmith'}),\n",
       " Document(page_content='data that cannot be logged. How can I ensure that only my team can access it?\\u200bIf you are interested in a private deployment of LangSmith or if you need to self-host, please reach out to us at sales@langchain.dev. Self-hosting LangSmith requires an annual enterprise license that also comes with support and formalized access to the LangChain team.Was this page helpful?NextUser GuideIntroductionInstall LangSmithCreate an API keySetup your environmentLog your first traceCreate your first', metadata={'description': 'Introduction', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'Getting started with LangSmith | ğŸ¦œï¸ğŸ› ï¸ LangSmith'})]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = vectorstore.as_retriever(k=4)\n",
    "\n",
    "docs = retriever.invoke(\"how can langsmith help with testing?\")\n",
    "\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "chat = ChatOpenAI(model=\"gpt-3.5-turbo-1106\")\n",
    "\n",
    "question_answering_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"Answer the user's questions based on the below context:\\n\\n{context}\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "document_chain = create_stuff_documents_chain(chat, question_answering_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LangSmith can help with testing by providing tracing and evaluation capabilities for your application. It allows you to closely monitor and evaluate your application during testing, so you can ship quickly and with confidence. Additionally, LangSmith offers a Prompt Hub, a prompt management tool built into LangSmith, which can aid in testing and managing prompts within your application. If you have specific questions about testing with LangSmith, feel free to reach out to the LangChain team at sales@langchain.dev for more information.'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo_ephemeral_chat_history = ChatMessageHistory()\n",
    "\n",
    "demo_ephemeral_chat_history.add_user_message(\"how can langsmith help with testing?\")\n",
    "\n",
    "document_chain.invoke(\n",
    "    {\n",
    "        \"messages\": demo_ephemeral_chat_history.messages,\n",
    "        \"context\": docs,\n",
    "    }\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
